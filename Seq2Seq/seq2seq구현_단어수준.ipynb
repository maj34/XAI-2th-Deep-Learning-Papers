{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b1dab3",
   "metadata": {},
   "source": [
    "# 단어 수준의 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41e638",
   "metadata": {},
   "source": [
    "## 참고\n",
    "\n",
    "https://wikidocs.net/86900\n",
    "\n",
    "\n",
    "## 데이터 출처\n",
    "\n",
    "http://www.manythings.org/anki/  \n",
    "fra-eng.zip의 fra.txt 파일을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7648fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8b5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20000 # 2만개의 샘플만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2061e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb417ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "    # 프랑스어 악센트(accent) 삭제\n",
    "    # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    # 악센트 제거 함수 호출\n",
    "    sent = to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백 추가.\n",
    "    # ex) \"I am a student.\" => \"I am a student .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24710f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e259ee",
   "metadata": {},
   "source": [
    "전체 2만개의 샘플에 대해 전처리를 수행하고 훈련 과정에서는 교사 강요를 사용.  \n",
    "입력 시퀀스에는 시작을 의미하는 \\<sos\\>토큰, 출력 시퀀스에는 종료를 의미하는 \\<eos\\>토큰을 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f44feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "    \n",
    "    with open(\"fra.txt\", \"r\", encoding='utf-8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6af6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# 3개의 데이터 셋 인코더의 입력, 디코더의 입력, 디코더의 레이블\n",
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaaacfc",
   "metadata": {},
   "source": [
    "글자 수준의 seq2seq과 마찬가지로 훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고 이전 시점의 실제 값을 넣어준다. 따라서 sents_fra_in이 필요하다!! (교사 강요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010015c",
   "metadata": {},
   "source": [
    "### 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68755445",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce60be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (20000, 7)\n",
      "디코더의 입력의 크기(shape) : (20000, 15)\n",
      "디코더의 레이블의 크기(shape) : (20000, 15)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17f7f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 3368, 프랑스어 단어 집합의 크기 : 6030\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합의 크기\n",
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38869b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어로부터 정수를 얻는 딕셔너리, 정수로부터 단어를 얻는 딕셔너리\n",
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa169e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [ 3097  9811 18795 ...  4343 10017 13258]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터로 분리 전 데이터 섞기 ( 순서가 섞인 정수 시퀀스 리스트)\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce38818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 섞기\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bedf8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6   9 373   1   0   0   0]\n",
      "[  2   8 220   1   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  8 220   1   3   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 출력\n",
    "print(encoder_input[10721])\n",
    "print(decoder_input[10721])\n",
    "print(decoder_target[10721])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc19fd",
   "metadata": {},
   "source": [
    "8, 220, 1 이라는 동일 시퀀스 확인 ( 2와 3은 \\<sos\\>, \\<eos\\> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05fbae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 2000\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(20000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2f17b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af2c253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (18000, 7)\n",
      "훈련 target 데이터의 크기 : (18000, 15)\n",
      "훈련 target 레이블의 크기 : (18000, 15)\n",
      "테스트 source 데이터의 크기 : (2000, 7)\n",
      "테스트 target 데이터의 크기 : (2000, 15)\n",
      "테스트 target 레이블의 크기 : (2000, 15)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca1d0e6",
   "metadata": {},
   "source": [
    "## 기계 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8375856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "673b9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba05ec9",
   "metadata": {},
   "source": [
    "인코더 설계\n",
    "- Masking은 연산을 제외하는 역할\n",
    "- 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state = True\n",
    "- state_h : hidden state, state_c : cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd31416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb475a3",
   "metadata": {},
   "source": [
    "다중 클래스 분류 문제이므로 출력층으로 소프트맥스 함수와 손실 함수를 크로스 엔트로피 함수를 사용한다. 크로스 엔트로피를 사용하기 위해서 레이블은 원-핫 인코딩이 되어야 한다. 이 때 원-핫 인코딩이 되지 않은 정수 레이블에 대한 다중 클래스 분류 문제를 풀고 싶을 때는 categorical_crossentropy 대신 sparse_categorical_crossentropy를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c2a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03c1b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "141/141 [==============================] - 18s 69ms/step - loss: 4.0931 - acc: 0.5985 - val_loss: 2.1721 - val_acc: 0.6133\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 7s 52ms/step - loss: 2.0266 - acc: 0.6176 - val_loss: 1.9022 - val_acc: 0.6381\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 7s 51ms/step - loss: 1.8016 - acc: 0.6913 - val_loss: 1.7415 - val_acc: 0.7372\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 7s 52ms/step - loss: 1.6653 - acc: 0.7423 - val_loss: 1.6256 - val_acc: 0.7460\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 7s 52ms/step - loss: 1.5536 - acc: 0.7531 - val_loss: 1.5202 - val_acc: 0.7620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb5978dcc8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86b4f5",
   "metadata": {},
   "source": [
    "## seq2seq Running\n",
    "\n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 hidden state와 cell state를 얻음\n",
    "2. 2가지의 state와 \\<sos\\>를 디코더로 보냄\n",
    "3. 디코더가 \\<eos\\>가 나올때까지 반복하여 다음 문자를 예측함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77950035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997b2a0",
   "metadata": {},
   "source": [
    "decode_sequence\n",
    "1. 입력 문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 hidden state와 cell state를 리턴\n",
    "2. 두 state를 states_value에 저장하고 디코더의 초기 입력으로 \\<sos\\>를 준비하여 target_seq에 저장\n",
    "3. 두 가지 입력을 가지고 while문으로 진입하여 두 가지를 디코더의 입력으로 사용\n",
    "4. 현재 시점에 대한 예측 시작 - 현재 시점의 예측 벡터는 output_tokens, 현재 시점의 hidden state는 h, 현재 시점의 cell state는 c, 현재 시점의 예측 단어는 target_seq으로 while문의 루프를 돌며 현재 시점의 예측 단어로 \\<eos\\>를 예측하거나 번역 문장의 길이가 50이 넘을 때까지 반복한다. 각 시점마다 번역한 단어는 decoded_sentence에 누적하여 저장하였다가 최종 번역 시퀀스로 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94e4f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "            len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e78eab",
   "metadata": {},
   "source": [
    "결과확인을 위한 함수 seq_to_src, seq_to_tar  \n",
    "영어 문장에 해당하는 정수 시퀀스를 입력받으면 영어 문장으로 변환, seq_to_tar은 프랑스어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "611049bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "    return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "            sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a16adff",
   "metadata": {},
   "source": [
    "훈련 데이터에서 임의로 선택한 인덱스의 샘플 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbfae9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : i felt fear . \n",
      "정답문장 : j ai ressenti de la peur . \n",
      "번역문장 : je ai ai . \n",
      "--------------------------------------------------\n",
      "입력문장 : can you show me ? \n",
      "정답문장 : peux tu me montrer ? \n",
      "번역문장 : tom est . \n",
      "--------------------------------------------------\n",
      "입력문장 : i got goosebumps . \n",
      "정답문장 : j ai la chair de poule . \n",
      "번역문장 : je ai ai . \n",
      "--------------------------------------------------\n",
      "입력문장 : class dismissed . \n",
      "정답문장 : le cours est termine . \n",
      "번역문장 : c est . \n",
      "--------------------------------------------------\n",
      "입력문장 : you re early . \n",
      "정답문장 : vous etes en avance . \n",
      "번역문장 : tom est . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c8c99",
   "metadata": {},
   "source": [
    "테스트 데이터에서 임의로 선택한 인덱스의 샘플 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3ea7a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : you re forgiven . \n",
      "정답문장 : tu es pardonne . \n",
      "번역문장 : tom est . \n",
      "--------------------------------------------------\n",
      "입력문장 : we did it . \n",
      "정답문장 : nous l avons fait . \n",
      "번역문장 : tom est a . \n",
      "--------------------------------------------------\n",
      "입력문장 : i like this room . \n",
      "정답문장 : j aime cette chambre . \n",
      "번역문장 : je ai ai . \n",
      "--------------------------------------------------\n",
      "입력문장 : i have standards . \n",
      "정답문장 : j ai des etendards . \n",
      "번역문장 : je ai ai . \n",
      "--------------------------------------------------\n",
      "입력문장 : that was fast . \n",
      "정답문장 : ca a ete rapide . \n",
      "번역문장 : tom est . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
