{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff16e1ad",
   "metadata": {},
   "source": [
    "# 글자 수준의 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd9f38",
   "metadata": {},
   "source": [
    "## 참고\n",
    "\n",
    "https://wikidocs.net/24996\n",
    "\n",
    "\n",
    "## 데이터 출처\n",
    "\n",
    "http://www.manythings.org/anki/  \n",
    "fra-eng.zip의 fra.txt 파일을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce779c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99629ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http = urllib3.PoolManager()\n",
    "# url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "# filename = 'fra-eng.zip'\n",
    "# path = os.getcwd()\n",
    "# zipfilename = os.path.join(path, filename)\n",
    "# with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "#     shutil.copyfileobj(r, out_file)\n",
    "\n",
    "# with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4516df",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811bd98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 194513\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1f132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194508</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194509</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194510</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194511</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194512</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194513 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      src  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Go.   \n",
       "3                                                     Hi.   \n",
       "4                                                     Hi.   \n",
       "...                                                   ...   \n",
       "194508  A carbon footprint is the amount of carbon dio...   \n",
       "194509  Death is something that we're often discourage...   \n",
       "194510  Since there are usually multiple websites on a...   \n",
       "194511  If someone who doesn't know your background sa...   \n",
       "194512  It may be impossible to get a completely error...   \n",
       "\n",
       "                                                      tar  \n",
       "0                                                    Va !  \n",
       "1                                                 Marche.  \n",
       "2                                                 Bouge !  \n",
       "3                                                 Salut !  \n",
       "4                                                  Salut.  \n",
       "...                                                   ...  \n",
       "194508  Une empreinte carbone est la somme de pollutio...  \n",
       "194509  La mort est une chose qu'on nous décourage sou...  \n",
       "194510  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "194511  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "194512  Il est peut-être impossible d'obtenir un Corpu...  \n",
       "\n",
       "[194513 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines # 영어 - 프랑스어 형태로 되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e0612",
   "metadata": {},
   "source": [
    "19만 4513개의 데이터로 이루어져있지만 간단히 20000개의 샘플 데이터만 가지고 기계 번역기를 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0771e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>I bowed politely.</td>\n",
       "      <td>Je m'inclinai poliment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>We agreed.</td>\n",
       "      <td>Nous sommes tombés d'accord.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>I'm at home.</td>\n",
       "      <td>Je suis dans la maison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>I could be next.</td>\n",
       "      <td>Je pourrais être le prochain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321</th>\n",
       "      <td>I fell in love.</td>\n",
       "      <td>Je suis tombée amoureuse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>We're dizzy.</td>\n",
       "      <td>On a la tête qui tourne.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>Who disagreed?</td>\n",
       "      <td>Qui n'était pas d’accord ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12364</th>\n",
       "      <td>We have a pool.</td>\n",
       "      <td>Nous avons une piscine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16979</th>\n",
       "      <td>You're annoying.</td>\n",
       "      <td>Tu es chiant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>I'm lucky.</td>\n",
       "      <td>Je suis chanceux.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     src                            tar\n",
       "18282  I bowed politely.        Je m'inclinai poliment.\n",
       "1563          We agreed.   Nous sommes tombés d'accord.\n",
       "3357        I'm at home.        Je suis dans la maison.\n",
       "13873   I could be next.  Je pourrais être le prochain.\n",
       "10321    I fell in love.      Je suis tombée amoureuse.\n",
       "4161        We're dizzy.       On a la tête qui tourne.\n",
       "9267      Who disagreed?     Qui n'était pas d’accord ?\n",
       "12364    We have a pool.        Nous avons une piscine.\n",
       "16979   You're annoying.                  Tu es chiant.\n",
       "1182          I'm lucky.              Je suis chanceux."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:20000] # 2만개만 저장\n",
    "lines.sample(10) # 랜덤으로 선택된 10개의 샘플"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd69c1",
   "metadata": {},
   "source": [
    "번역 문장에 해당하는 프랑스어 데이터에는 시작을 의미하는 심볼 \\<sos\\>과 종료를 의미하는 심볼 \\<eos\\>를 넣어주어야 한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcbbc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17053</th>\n",
       "      <td>You're involved.</td>\n",
       "      <td>\\t Tu es impliqué. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15677</th>\n",
       "      <td>The moon is out.</td>\n",
       "      <td>\\t La lune est visible. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18381</th>\n",
       "      <td>I didn't see you.</td>\n",
       "      <td>\\t Je ne vous ai pas vu. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18763</th>\n",
       "      <td>I made it myself.</td>\n",
       "      <td>\\t Je l'ai fait moi-même. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19668</th>\n",
       "      <td>It's your choice.</td>\n",
       "      <td>\\t C'est ton choix. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17476</th>\n",
       "      <td>Did you love Tom?</td>\n",
       "      <td>\\t Aimiez-vous Tom ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>I like picnics.</td>\n",
       "      <td>\\t J'aime bien les pique-niques. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12136</th>\n",
       "      <td>Tom looks dead.</td>\n",
       "      <td>\\t Tom semble être mort. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>I'll study a lot.</td>\n",
       "      <td>\\t J'étudierai beaucoup. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19122</th>\n",
       "      <td>I'll go with you.</td>\n",
       "      <td>\\t Je vais y aller avec toi. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     src                                  tar\n",
       "17053   You're involved.                \\t Tu es impliqué. \\n\n",
       "15677   The moon is out.           \\t La lune est visible. \\n\n",
       "18381  I didn't see you.          \\t Je ne vous ai pas vu. \\n\n",
       "18763  I made it myself.         \\t Je l'ai fait moi-même. \\n\n",
       "19668  It's your choice.               \\t C'est ton choix. \\n\n",
       "17476  Did you love Tom?              \\t Aimiez-vous Tom ? \\n\n",
       "10502    I like picnics.  \\t J'aime bien les pique-niques. \\n\n",
       "12136    Tom looks dead.          \\t Tom semble être mort. \\n\n",
       "19141  I'll study a lot.          \\t J'étudierai beaucoup. \\n\n",
       "19122  I'll go with you.      \\t Je vais y aller avec toi. \\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a565c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축 ( 문자 단위... A, B, C...)\n",
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 문자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245e2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 75\n",
      "target 문장의 char 집합 : 101\n"
     ]
    }
   ],
   "source": [
    "# 문자 집합의 크기 확인\n",
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7e793",
   "metadata": {},
   "source": [
    "지금 상태에서 인덱스를 사용해 일부만 출력하려고 하면 에러가 나기 때문에 정렬 후 사용해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33348242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정렬 전 (set형태)\n",
    "#print(src_vocab[45:75])\n",
    "#print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57482390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é']\n",
      "['U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06738674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'Y': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73, 'é': 74}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, ',': 11, '-': 12, '.': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'Y': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '\\xa0': 76, '«': 77, '»': 78, 'À': 79, 'Ç': 80, 'É': 81, 'Ê': 82, 'Ô': 83, 'à': 84, 'â': 85, 'ç': 86, 'è': 87, 'é': 88, 'ê': 89, 'ë': 90, 'î': 91, 'ï': 92, 'ô': 93, 'ù': 94, 'û': 95, 'œ': 96, '\\u2009': 97, '‘': 98, '’': 99, '\\u202f': 100}\n"
     ]
    }
   ],
   "source": [
    "# 각 문자에 인덱스 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c383c2",
   "metadata": {},
   "source": [
    "인덱스가 부여된 문자 집합을 이용해 훈련 데이터를 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "906415ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 62, 10], [30, 62, 10], [30, 62, 10], [31, 56, 10], [31, 56, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 인코더에 입력될 english data\n",
    "encoder_input = []\n",
    "\n",
    "# 1개의 문장\n",
    "for line in lines.src:\n",
    "    encoded_line = []\n",
    "    # 각 줄에서 1개의 char\n",
    "    for char in line:\n",
    "        # 각 char을 정수로 변환\n",
    "        encoded_line.append(src_to_index[char]) # 딕셔너리의 키를 이용해 숫자로 변환\n",
    "    encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359a4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 3, 47, 50, 3, 4, 3, 2], [1, 3, 38, 50, 67, 52, 57, 54, 13, 3, 2], [1, 3, 27, 64, 70, 56, 54, 3, 4, 3, 2], [1, 3, 44, 50, 61, 70, 69, 3, 4, 3, 2], [1, 3, 44, 50, 61, 70, 69, 13, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더에 입력될 French data\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        encoded_line.append(tar_to_index[char])\n",
    "    decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :',decoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df2db5",
   "metadata": {},
   "source": [
    "정상적으로 정수 인코딩이 수행되었다.  \n",
    "decoder_input의 결과를 살펴보면 시작에 \\<sos\\>, \\<eos\\>(\\t, \\n)를 넣었기 때문에 앞과 뒤의 1개의 숫자가 모두 같은 것을 확인할 수 있다.  \n",
    "이제 디코더의 예측값과 비교하기 위한 실제값이 필요한데, 실제 값에는 시작 심볼에 해당하는 \\<sos\\>가 있을 필요가 없다. 따라서 정수 인코딩과정에서 \\<sos\\>를 제거해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f94dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[3, 47, 50, 3, 4, 3, 2], [3, 38, 50, 67, 52, 57, 54, 13, 3, 2], [3, 27, 64, 70, 56, 54, 3, 4, 3, 2], [3, 44, 50, 61, 70, 69, 3, 4, 3, 2], [3, 44, 50, 61, 70, 69, 13, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    timestep = 0 \n",
    "    encoded_line = []\n",
    "    for char in line:\n",
    "        # 앞의 1글자(<sos>)를 제외하고 append\n",
    "        if timestep > 0:\n",
    "            encoded_line.append(tar_to_index[char])\n",
    "        timestep = timestep + 1\n",
    "    decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83420da4",
   "metadata": {},
   "source": [
    "decoder_input과 decoder_target을 비교해보면 decoder_target에서 \\<sos\\>에 해당하는 5개의 정수가 사라진 것을 확인할 수 있다.\n",
    "\n",
    "다음은 패딩 작업을 수행한다. 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dee8767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 17\n",
      "target 문장의 최대 길이 : 61\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b81e7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어는 영어 데이터끼리 프랑스어는 프랑스어끼리 길이에 맞춰 패딩\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9764466e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 17), (20000, 61), (20000, 61))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, decoder_input.shape, decoder_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f51bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 값에 대해 원-핫 인코딩 수행\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebd3e74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 17, 75), (20000, 61, 101), (20000, 61, 101))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, decoder_input.shape, decoder_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a8886",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**현재 시점의 디코더 셀의 입력은 이전 시점의 디코더 셀의 출력을 입력받는다고 했는데 decoder_input이 필요한 이유?** : Training 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력값으로 넣어주지 않고 이전 시점의 실제 값을 현재 시점의 디코더 셀의 입력값으로 넣어줄 것이다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 연쇄 작용으로 디코더 전체의 예측이 어려워질 수 있기 때문이다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제 값을 입력으로 주는 방법을 교사 강요(Teacher forcing)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "689334b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3484066",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "# encoder_outputs은 여기서는 불필요\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dc87172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b9d12",
   "metadata": {},
   "source": [
    "인코더를 살펴보면 LSTM의 hidden state는 256으로, 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정\n",
    "\n",
    "LSTM에서는 state_h, state_c를 리턴받는데 이는 각각 hidden state와 cell state를 의미한다.두 state는 모두 디코더로 전달된다. (Context Vector)\n",
    "\n",
    "디코더는 인코더의 마지막 hidden state를 초기 hidden state로 사용한다. 디코더도 hidden state와 cell state를 리턴하기는 하지만 Training과정에서는 사용하지 않는다. 이후 출력층에 프랑스어 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 오차를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83908887",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "# return_sequences : 시퀀스 출력 여부 \n",
    "# many to many 문제 or LSTM layer를 여러개로 쌓아올릴 때 사용한다.\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달. ( 이전 결과를 저장하지 않는 모습)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e844207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 14s 44ms/step - loss: 1.0769 - val_loss: 0.8695\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.6506 - val_loss: 0.7086\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.5464 - val_loss: 0.6218\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.4912 - val_loss: 0.5838\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.4511 - val_loss: 0.5458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1923704c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, \n",
    "          epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6660b6",
   "metadata": {},
   "source": [
    "위에서 설정한 hidden_state의 크기와 epoch 수는 train date에 과적합을 만든다. 따라서 중간부터 val_loss값이 올라가는데 이번 구현에서는 주어진 데이터의 양과 태스크의 특성 상 훈련 데이터의 정확도와 과적합 방지 두 가지를 잡기가 쉽지 않기 때문에 seq2seq 매커니즘을 구현해보고 짧은 문장과 긴 문장에 대한 성능 차이를 확인하는 것에 중점을 둔다.\n",
    "\n",
    "컴퓨터 사양의 한계로 본 예시와는 다르게 2만개의 데이터로, epoch를 5로 낮추어 진행하였다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b70f1",
   "metadata": {},
   "source": [
    "## Running\n",
    "\n",
    "seq2seq은 훈련할 때와 동작할 때의 방식이 다르다.  \n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 hidden state와 cell state를 얻음\n",
    "2. 2가지의 state와 \\<sos\\>를 디코더로 보냄\n",
    "3. 디코더가 \\<eos\\>가 나올때까지 반복하여 다음 문자를 예측함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f6a9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60dbce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 정의\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a29e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800c0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "            len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66b1d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Hi.\n",
      "정답 문장: Salut ! \n",
      "번역 문장: Allez ! \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장: Salut ! \n",
      "번역 문장: Arrêtez ! \n",
      "-----------------------------------\n",
      "입력 문장: Hop in.\n",
      "정답 문장: Montez. \n",
      "번역 문장: Arrêtez ! \n",
      "-----------------------------------\n",
      "입력 문장: Help me!\n",
      "정답 문장: Aide-moi ! \n",
      "번역 문장: Arrêtez ! \n",
      "-----------------------------------\n",
      "입력 문장: Humor Tom.\n",
      "정답 문장: Mettez Tom de bonne humeur. \n",
      "번역 문장: Arrêtez de conter ! \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
